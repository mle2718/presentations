@article{McConnell2000,
author = {Mcconnell, Kenneth E and Strand, Ivar E},
journal = {American Journal of Agricultural Economics},
number = {1},
pages = {133--144},
publisher = {American Agricultural Economics Association},
title = {{Hedonic Prices for Fish: Tuna Prices in Hawaii}},
volume = {82},
year = {2000}
}

@article{Ardini2018,
abstract = {Management of the " General Category " component of the US Atlantic sea scallop (Placopecten magellanicus) fishery changed from open access with a soft, or target, catch limit, to limited access with a hard catch limit, to individual fishing quotas (IFQs) in just three years. Two differences-in-differences (DiD) models are used to examine the causal effects of management on price and revenue. A hedonic price model finds that the IFQ program had minimal direct effects on prices. A landings composition model finds that the IFQ program increased landings of the largest scallops. The Limited Access fleet, which lands most of the scallops in the region and is managed primarily with input controls, serves as a control for both models. Our policy simulation finds that IFQs increased revenues by 2.6% compared to hard catch limits. However, the IFQ program did not increase revenues relative to the regulated open-access system.},
author = {Ardini, Greg and Lee, Min-Yang},
doi = {10.1086/698199},
journal = {Marine Resource Economics},
keywords = {Atlantic sea scallop,D40,Q22,Q58,differences-in-differences,individual fishing quotas JEL Codes},
number = {3},
pages = {263--288},
title = {{Do IFQs in the US Atlantic Sea Scallop Fishery Impact Price and Size?}},
volume = {33},
year = {2018}
}


@article{Kristofersson2007,
abstract = {The price of a product depends on its characteristics and will vary in dynamic markets. The model describes a processing firm that bids in an auction for a heterogeneous and perishable input. The reduced form of this model is estimated as an expanded random parameter model that combines a nonlinear hedonic bid function and inverse input demand functions for characteristics. The model was estimated by using 289,405 transactions from the Icelandic fish auctions. Total catch and gut ratio were the main determinants of marginal prices of characteristics, while the price of cod mainly depended on size, gutting and storage.},
author = {Kristofersson, Dadi and Rickertsen, Kyrre},
doi = {10.1111/j.1468-0084.2006.00441.x},
isbn = {0305-9049},
journal = {Oxford Bulletin of Economics and Statistics},
number = {3},
pages = {387--412},
title = {{Hedonic price models for dynamic markets}},
volume = {69},
year = {2007}
}

@article{Hammarlund2015,
author = {Hammarlund, Cecilia},
journal = {Marine Resource Economics},
number = {2},
pages = {157--177},
publisher = {University of Chicago Press Chicago, IL},
title = {{The big, the bad, and the average: Hedonic prices and inverse demand for Baltic cod}},
volume = {30},
year = {2015}
}


@book{Breiman1984,
author = {Breiman, Leo and Friedman, Jerome and Olshen, R.A. and Stone, Charles},
doi = {https://doi.org/10.1201/9781315139470},
isbn = {9780412048418},
pages = {368},
publisher = {Routledge},
title = {{Classification and regression trees}},
year = {1984}
}

@article{Breiman2001,
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the corre- lation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth Interna- tional conference, ∗∗∗, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
author = {Breiman, Leo},
journal = {Machine Learning},
keywords = {classification,ensemble,regression},
pages = {5--32},
title = {{Random Forests}},
volume = {45},
year = {2001}
}


@article{Bates2024,
abstract = {Cross-validation is a widely used technique to estimate prediction error, but its behavior is complex and not fully understood. Ideally, one would like to think that cross-validation estimates the prediction error for the model at hand, fit to the training data. We prove that this is not the case for the linear model fit by ordinary least squares; rather it estimates the average prediction error of models fit on other unseen training sets drawn from the same population. We further show that this phenomenon occurs for most popular estimates of prediction error, including data splitting, bootstrapping, and Mallow's (Formula presented.). Next, the standard confidence intervals for prediction error derived from cross-validation may have coverage far below the desired level. Because each data point is used for both training and testing, there are correlations among the measured accuracies for each fold, and so the usual estimate of variance is too small. We introduce a nested cross-validation scheme to estimate this variance more accurately, and show empirically that this modification leads to intervals with approximately correct coverage in many examples where traditional cross-validation intervals fail. Lastly, our analysis also shows that when producing confidence intervals for prediction accuracy with simple data splitting, one should not refit the model on the combined data, since this invalidates the confidence intervals. Supplementary materials for this article are available online.},
archivePrefix = {arXiv},
arxivId = {2104.00673},
author = {Bates, Stephen and Hastie, Trevor and Tibshirani, Robert},
doi = {10.1080/01621459.2023.2197686},
eprint = {2104.00673},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Bootstrap/resampling,Computationally intensive methods,Cross-validation,Goodness-of-fit methods},
number = {546},
pages = {1434--1445},
title = {{Cross-Validation: What Does It Estimate and How Well Does It Do It?}},
volume = {119},
year = {2024}
}



@book{Hastie2009,
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
doi = {10.1111/j.1532-5415.1984.tb02220.x},
issn = {15325415},
pages = {745},
pmid = {6725805},
publisher = {Springer},
title = {{The Elements of Statistical Learning: Data Mining, Inference, and Prediction}}
}


@article{Varian2014,
author = {Varian, Hal R.},
doi = {10.1257/jep.28.2.3},
issn = {08953309},
journal = {Journal of Economic Perspectives},
number = {2},
pages = {3--28},
title = {{Big data: New tricks for econometrics}},
volume = {28},
year = {2014}
}


@book{Kuhn2013a,
abstract = {Applied Predictive Modeling covers the overall predictive modeling process, beginning with the crucial steps of data preprocessing, data splitting and foundations of model tuning. The text then provides intuitive explanations of numerous common and modern regression and classification techniques, always with an emphasis on illustrating and solving real data problems. The text illustrates all parts of the modeling process through many hands-on, real-life examples, and every chapter contains extensive R code for each step of the process. This multi-purpose text can be used as an introduction to predictive models and the overall modeling process, a practitioner's reference handbook, or as a text for advanced undergraduate or graduate level predictive modeling courses. To that end, each chapter contains problem sets to help solidify the covered concepts and uses data available in the book's R package. This text is intended for a broad audience as both an introduction to predictive models as well as a guide to applying them. Non-mathematical readers will appreciate the intuitive explanations of the techniques while an emphasis on problem-solving with real data across a wide variety of applications will aid practitioners who wish to extend their expertise. Readers should have knowledge of basic statistical ideas, such as correlation and linear regression analysis. While the text is biased against complex equations, a mathematical background is needed for advanced topics.},
author = {Kuhn, Max and Johnson, Kjell},
booktitle = {Springer},
isbn = {9781461468486},
pages = {615},
title = {{Applied Predictive Modeling with Applications in R}},
url = {http://appliedpredictivemodeling.com/s/Applied_Predictive_Modeling_in_R.pdf},
volume = {26},
year = {2013}
}

@article{Athey2017,
archivePrefix = {arXiv},
arxivId = {1607.00699},
author = {Athey, Susan and Imbens, Guido W.},
doi = {10.1257/jep.31.2.3},
eprint = {1607.00699},
issn = {08953309},
journal = {Journal of Economic Perspectives},
number = {2},
pages = {3--32},
title = {{The state of applied econometrics: Causality and policy evaluation}},
volume = {31},
year = {2017}
}


@article{Storm2020,
abstract = {This review presents machine learning (ML) approaches from an applied economist's perspective. We first introduce the key ML methods drawing connections to econometric practice. We then identify current limitations of the econometric and simulation model toolbox in applied economics and explore potential solutions afforded by ML. We dive into cases such as inflexible functional forms, unstructured data sources and large numbers of explanatory variables in both prediction and causal analysis, and highlight the challenges of complex simulation models. Finally, we argue that economists have a vital role in addressing the shortcomings of ML when used for quantitative economic analysis.},
author = {Storm, Hugo and Baylis, Kathy and Heckelei, Thomas},
doi = {10.1093/erae/jbz033},
issn = {14643618},
journal = {European Review of Agricultural Economics},
keywords = {agri-environmental policy analysis,econometrics,machine learning,quantitative economic analysis,simulation models},
number = {3},
pages = {849--892},
title = {{Machine learning in agricultural and applied economics}},
volume = {47},
year = {2020}
}